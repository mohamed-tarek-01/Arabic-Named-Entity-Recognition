import streamlit as st
import pickle
import numpy as np
import pandas as pd
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import Counter
import re
import random
import matplotlib.pyplot as plt 
import base64
import os
from PIL import Image

@st.cache_resource
def load_files():    
    model = load_model("ner_model.keras")
    with open("word2idx.pkl", "rb") as f:
        word2idx = pickle.load(f)
    with open("tag2idx.pkl", "rb") as f:
        tag2idx = pickle.load(f)
    with open("tags.pkl", "rb") as f:
        tags = pickle.load(f)
    with open("punctuations.pkl", "rb") as f:
        punctuations = pickle.load(f)
    with open("stop_words.pkl", "rb") as f:
        stop_words = pickle.load(f)

    idx2tag = {i: t for t, i in tag2idx.items()}
    return model, word2idx, tag2idx, tags, idx2tag, punctuations, stop_words

model, word2idx, tag2idx, tags, idx2tag, punctuations, stop_words = load_files()

def predict(text, model, word2idx, tags, stop_words, punctuations):
    processed_words = []
    removed_words = []

    for word in text.split():
        original_word = word

        word = re.sub(rf"[{re.escape(''.join(punctuations))}]", "", word)

        non_arabic_parts = re.findall(r'[^\u0600-\u06FF]+', word)
        if non_arabic_parts:
            for part in non_arabic_parts:
                removed_words.append((part, "Non-Arabic"))
            word = re.sub(r'[^\u0600-\u06FF]', '', word)

        if word in stop_words:
            removed_words.append((original_word, "Stopword"))
            continue

        word = re.sub(r'\d+', '', word)

        if len(word) == 1 and word != '.':
            removed_words.append((original_word, "Too Short"))
            continue

        if word.startswith('Ø§Ù„') and len(word) > 2:
            word = word[2:]

        word = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', word)
        word = re.sub(r'Ù‰', 'ÙŠ', word)
        word = re.sub(r'Ø¤', 'Ùˆ', word)
        word = re.sub(r'Ø¦', 'ÙŠ', word)
        word = re.sub(r'Ø©', 'Ù‡', word)
        word = re.sub(r'[ÙÙ‹ÙÙŒÙÙÙ’Ù‘]', '', word)

        if word.strip():
            processed_words.append(word)
        else:
            continue

    if not processed_words:
        return [], processed_words, removed_words

    word_indices = [word2idx.get(w, word2idx.get("ENDPAD", 0)) for w in processed_words]
    word_indices = pad_sequences([word_indices], maxlen=100, padding='post', value=word2idx.get("ENDPAD", 0))
    
    pred = model.predict(np.array(word_indices))
    pred_tags = np.argmax(pred, axis=-1)[0]
    
    result = [(word, tags[tag_idx]) for word, tag_idx in zip(processed_words, pred_tags[:len(processed_words)])]
    
    return result, processed_words, removed_words

    word_indices = [word2idx.get(w, word2idx.get("ENDPAD", 0)) for w in processed_words]
    word_indices = pad_sequences([word_indices], maxlen=100, padding='post', value=word2idx.get("ENDPAD", 0))
    
    pred = model.predict(np.array(word_indices))
    pred_tags = np.argmax(pred, axis=-1)[0]

    result = [(word, tags[tag_idx]) for word, tag_idx in zip(processed_words, pred_tags[:len(processed_words)])]
    
    return result, processed_words, removed_words

st.title(" Arabic Named Entity Recognition (NER)")

tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” Analysis", "ğŸ“Š Statistics", "ğŸ§° Preprocessing"])

with tab1:
    st.markdown("## âœï¸ Enter Text or Use Sample")

    political_samples = [
    "Ø§Ù„Ø±Ø¦ÙŠØ³ Ø¹Ø¨Ø¯ Ø§Ù„ÙØªØ§Ø­ Ø§Ù„Ø³ÙŠØ³ÙŠ Ø²Ø§Ø± Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©.",
    "Ø§Ù„Ø±Ø¦ÙŠØ³ Ø§Ù„Ø§Ù…Ø±ÙŠÙƒÙŠ Ø£ÙƒØ¯ Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø¨Ø¯Ø¹Ù… Ø­Ù„ÙØ§Ø¦Ù‡Ø§ ÙÙŠ Ø­Ù„Ù Ø§Ù„Ù†Ø§ØªÙˆ Ø®Ù„Ø§Ù„ Ù‚Ù…Ø© Ø¨Ø±ÙˆÙƒØ³Ù„.",
    "Ù…Ø¤ØªÙ…Ø± Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± Ø´Ù‡Ø¯ Ø­Ø¶ÙˆØ± Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø²Ø¹Ù…Ø§Ø¡ Ø§Ù„Ø¹Ø±Ø¨ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ù‚Ø¶Ø§ÙŠØ§ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©.",
    "Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø¯Ø¹Øª Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø§Øª Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ© Ù„Ø´Ø¹Ø¨ ÙÙ„Ø³Ø·ÙŠÙ† ÙÙŠ ØºØ²Ø© Ø¨Ø¹Ø¯ ØªØµØ§Ø¹Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø¹Ø¯Ø§Ø¦ÙŠØ©.",
    "Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠÙ‡ Ø£Ù†Ø¬ÙŠÙ„Ø§ Ù…ÙŠØ±ÙƒÙ„ Ø£Ø¹Ù„Ù†Øª Ø¹Ù† Ø®Ø·Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù…ÙƒØ§ÙØ­Ø© Ø£Ø²Ù…Ø© Ø§Ù„Ù‡Ø¬Ø±Ø© ÙÙŠ Ø£Ù„Ù…Ø§Ù†ÙŠØ§  Ùˆ Ø£ÙˆØ±ÙˆØ¨Ø§."
    ]

    sports_samples = [
        "Ù…Ù†ØªØ®Ø¨ Ù…ØµØ± Ù„ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… ØªØ£Ù‡Ù„ Ø¥Ù„Ù‰ ÙƒØ£Ø³ Ø§Ù„Ø¹Ø§Ù„Ù… Ø¨Ø¹Ø¯ ÙÙˆØ²Ù‡ Ø¹Ù„Ù‰ Ù…Ù†ØªØ®Ø¨ ØªÙˆÙ†Ø³.",
        "Ø§Ù„Ù„Ø§Ø¹Ø¨ Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­ Ø³Ø¬Ù„ Ù‡Ø¯ÙÙ‹Ø§ Ø±Ø§Ø¦Ø¹Ù‹Ø§ ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ù„ÙŠÙØ±Ø¨ÙˆÙ„ Ø¶Ø¯ Ù…Ø§Ù†Ø´Ø³ØªØ± Ø³ÙŠØªÙŠ.",
        "ÙØ±ÙŠÙ‚ Ø§Ù„Ø²Ù…Ø§Ù„Ùƒ Ø­Ù‚Ù‚ Ø§Ù†ØªØµØ§Ø±Ù‹Ø§ Ø³Ø§Ø­Ù‚Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙŠ Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©.",
        "Ù„Ø§Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ù„ÙŠÙˆÙ†ÙŠÙ„ Ù…ÙŠØ³ÙŠ Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ù†Ø§Ø¯ÙŠ Ø¨Ø§Ø±ÙŠØ³ Ø³Ø§Ù† Ø¬ÙŠØ±Ù…Ø§Ù† Ø¨Ø¹Ø¯ ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ù…Ø¹ Ø¨Ø±Ø´Ù„ÙˆÙ†Ø©.",
        "Ù…Ù†ØªØ®Ø¨ Ù…ØµØ± Ù„ÙƒØ±Ø© Ø§Ù„ÙŠØ¯ ØªØ£Ù‡Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¯ÙˆØ± Ù†ØµÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙÙŠ Ø§Ù„Ø¨Ø·ÙˆÙ„Ø© Ø§Ù„Ø¥ÙØ±ÙŠÙ‚ÙŠØ©."
    ]

    education_samples = [
    "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ø£Ø¹Ù„Ù†Øª Ø¹Ù† ÙØªØ­ Ø¨Ø§Ø¨ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ù„Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¬Ø¯Ø¯ ÙÙŠ Ù…ØµØ± Ù„Ù„Ø¹Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ù…Ù‚Ø¨Ù„.",
    "Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙÙŠ Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© ØªÙˆÙØ± Ù…Ù†Ø­ Ø¯Ø±Ø§Ø³ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¯ÙˆÙ„ÙŠÙŠÙ† ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©.",
    "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ù„Ùƒ Ø³Ø¹ÙˆØ¯ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶ Ø£Ø·Ù„Ù‚Øª Ù…Ø±ÙƒØ²Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ·ÙˆÙŠØ± ÙÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù…ØªØ¬Ø¯Ø¯Ø©." ,
    "Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠØ© ÙÙŠ Ø¹Ù…Ø§Ù† Ø¨Ø¯Ø£Øª ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø¨Ø±Ø§Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ù…Ø´ØªØ±ÙƒØ© Ù…Ø¹ Ø¬Ø§Ù…Ø¹Ø© ÙƒØ§Ù„ÙŠÙÙˆØ±Ù†ÙŠØ§ ÙÙŠ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ù„ØªØ·ÙˆÙŠØ± Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©.",
    "Ø¬Ø§Ù…Ø¹Ø© Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø²Ø§ÙŠØ¯ ÙÙŠ Ø£Ø¨ÙˆØ¸Ø¨ÙŠ Ù‚Ø¯Ù…Øª Ù…Ù†Ø­Ù‹Ø§ Ø¯Ø±Ø§Ø³ÙŠØ© Ù„Ù„Ø·Ù„Ø§Ø¨ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· ÙÙŠ Ù…Ø¬Ø§Ù„ Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ."
    ]

    if "user_input" not in st.session_state:
        st.session_state["user_input"] = ""

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("ğŸ“° Political Sample"):
            selected_sample = random.choice(political_samples)
            st.session_state["user_input"] = selected_sample

    with col2:
        if st.button("ğŸŸï¸ Sports Sample"):
            selected_sample = random.choice(sports_samples)
            st.session_state["user_input"] = selected_sample

    with col3:
        if st.button("ğŸ« Education Sample"):
            selected_sample = random.choice(education_samples)
            st.session_state["user_input"] = selected_sample

    
    user_input = st.text_area("ğŸ“Enter Arabic Sentences:", height=150, key="user_input")

    if st.button("ğŸ” Analyze Entities"):
        if user_input.strip():
            st.subheader("ğŸ“„ Results:")
            entities,_,_ = predict(user_input, model, word2idx, tags, stop_words, punctuations)
            df_results = pd.DataFrame(entities, columns=["Word", "Tag"])
            st.markdown("### ğŸ§¾ Prediction Table")
            st.table(df_results)
            csv = df_results.to_csv(index=False).encode("utf-8")
            st.download_button("ğŸ“¥ Download Results as CSV", csv, "ner_results.csv", "text/csv")
            st.session_state["entities"] = entities
        else:
            st.warning("âš ï¸ Please enter some text first.")

with tab2:
    st.markdown("## ğŸ“Š Named Entity Statistics")

    if "entities" in st.session_state:
        entities = st.session_state["entities"]
        tags_only = [tag for _, tag in entities if tag != "O"]
        tag_counts = Counter(tags_only)
        total_entities = len(tags_only)
        st.subheader(f"Total Named Entities: {total_entities}")
        if tag_counts:
            chart_data = pd.DataFrame(
                tag_counts.values(),
                index=tag_counts.keys(),
                columns=["Count"]
            )
            st.markdown("### ğŸ“Š Entity Tag Distribution (Bar Chart)")
            st.bar_chart(chart_data)
        
            st.markdown("### ğŸ¥§ Entity Tag Distribution (Pie Chart)")
            pie_data = pd.Series(tag_counts).plot.pie(autopct="%.1f%%", colors=["#FF9999","#66B3FF","#99FF99","#FFCC99"], startangle=90, figsize=(7, 7))
            st.pyplot(pie_data.figure)

            st.markdown("### ğŸ“Š Entity Tag Counts")
            st.write(tag_counts)

        else:
            st.info("â• No named entities found to display.")
    else:
        st.info("â„¹ï¸ Run an analysis from the first tab.")

with tab3:
    st.markdown("## ğŸ§° Preprocessing Preview")

    user_input = st.session_state.get("user_input", "").strip()

    if user_input:
        original_text = st.session_state["user_input"]
        st.markdown("### ğŸ”¹ Original Text")
        st.info(original_text)
        _, cleaned_words, removed_words = predict(
            original_text, model, word2idx, tags, stop_words, punctuations
        )
        st.markdown("### âœ… Cleaned Text After Preprocessing")
        st.success(" ".join(cleaned_words) if cleaned_words else "No words left after cleaning.")

        if removed_words:
            st.markdown("### âŒ Removed / Filtered Words")
            df_removed = pd.DataFrame(removed_words, columns=["Word", "Reason"])
            st.table(df_removed)
        else:
            st.info("No words were removed during preprocessing.")
    else:
        st.info("â„¹ï¸ Enter some text from Tab 1 to preview preprocessing.")
